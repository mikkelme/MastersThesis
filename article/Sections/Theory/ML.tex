\chapter{Machine Learning}
In this thesis machine learning will serve as a numerical tool for evaluating
and exploring the frictional behavior of various Kirigami designs. We will
generate data using \acrshort{MD} simulations which serve as a ground truth for
the training of a machine learning model. If successful, we can utilize such a
model to predict the frictional behavior of unseen configurations. The machine
learning predictions will be a lot faster than carrying out a complete
\acrshort{MD} simulation and thus this can be used to accelerate the search
through a new set of configurations. It is not obvious that the machine learning
model can readily capture the physical mechanisms in our system. Hence, the
attempt to model the system with machine learning also has value in terms of
revealing the usefulness of such methods to this problem. We aim to implement a
rather traditional machine-learning approach. In this chapter we introduce the
key concept behind machine learning and some of the relevant concepts and
techniques relevant to our implementation.


\section{Neural network}
The neural network, or more precisely the \textit{feed forward dense neural
network}, is one of the original concepts in machine learning arising from the attempt of mimicking the way neurons work in the brain \cite{lederer2021activation, Shankar_2022}. The neural network can be considered as three major parts: The input layer, the
so-called \textit{hidden layers} and finally the output layer as shown in
\cref{fig:ffnn}. The input is described as a vector $\vec{x} = x_0, x_1, \ldots,
x_{n_x}$ where each input $x_i$ is usually denoted as a \textit{feature}. For
our task we will consider the Kirigami configuration, load and stretch of the system as input features on which we want the model to base its prediction. The
input features are densely connected to each of the \textit{nodes} in the first
hidden layers as indicated by straight lines in \cref{fig:ffnn}. Each line
represents a weighted connection that can be adjusted to configure the
importance of that feature. Similar dense connections run through all the
hidden layers to the final output layer. For a given note $a_j^{[l]}$ in layer $l$ will process the input from layer $l-1$ as
\begin{align*}
  a_j^{[l]} = f\left(\sum_i w^{[l]}_{ij}a_i^{[l-1]} + b_j^{[l]}\right),
\end{align*}
where $w^{[l]}_{ij}$ is the weight connection node $a_i^{[l-1]}$ of the previous layer to the node $a_j^{[l]}$ in the current layer. Note that the choice of denoting this weight to belong to layer $l$ as opposed $l-1$ is simply a notation choice. $b_j^{[l]}$ denotes a bias and $f(\cdot)$ the \textit{activation function}. The activation function is often chosen to give a non-linear mapping of the input to each node. Without this, the network will only be capable of approximate linear functions \cite{lederer2021activation}. Two common activation functions are the \textit{sigmoid}, mapping the input to the range $(0,1)$, and the ReLU which cuts off negative values and maps positive linearly
\begin{align*}
  \text{Sigmoid:} \quad f(z) = \frac{1}{1 + e^{-z}}, \qquad \qquad
  \text{ReLU:} \quad 
  f(z)= \begin{cases}
    z \text{ for } z > 0   \\
    0 \text{ for } z \leq 0.
    \end{cases}
\end{align*}
Often the same activation function is used throughout the network, except for the output layer where the activation function is usually omitted or the sigmoid is used for classification tasks. The whole process of sending data through the model is called \textit{forward propagation} and constitutes the mechanism for mapping an input $\vec{x}$ to the model output $\hat{vec{y}}$. In order to get useful predictions we must \textit{train} the model which involves tuning the model parameters, the weight and biasses.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/ffnn.png}
  \caption{\hl{From overleaf IN400}}
  \label{fig:ffnn}
\end{figure}


The model training relies on two core concepts: \textit{backpropagation} and \textit{gradient descent} optimization. We define the error associated with a model prediction, otherwise known as the \textit{loss}, through the \textit{loss function} $L$ that evaluates the model output  $\hat{\vec{y}}$ against the true value $\vec{y}$. For a continuous scalar output, we might simply use the mean squared error (\acrshort{MSE})
\begin{align*}
  L_{\text{\acrshort{MSE}}} = \frac{1}{N_y} \sum_{i = 1}^N (y_i - \hat{y}_i)^2.
\end{align*}
For a binary classification problem, meaning that the true output is True or False (1 or 0), a common choice is binary cross entropy (BSE)
\begin{align*}
  L_{\text{BSE}} =  -\sum_{i=1}^n \ \Big[y_i\log(\hat{y_i}) + (1-y_i)\log(1 - \hat{y_i}) \Big] =  \sum_{i=1}^n   \begin{cases}
    - \log{(\hat{y_i})},& \quad y_i = 1 \\
    -\log{(1-\hat{y_i})},& \quad y_i = 0.
\end{cases}
\end{align*}
The cross-entropy loss can be derived from a maximum likelihood estimation \hl{SOURCE}. Without going into details with the derivation we can convince ourselves that the error is minimized for the correct prediction and maximized for the worst prediction. When $y_i = 1$ we get the negative term $-\log(\hat{y_i})$ where a correct prediction $\hat{y}_i \to 1$ yields a loss contribution $L_i \to 0$. For a wrong prediction $\hat{y}_i \to 0$ the loss contribution will diverge $L_i \to \infty$. Similar applies to the case of $y_i = 0$ with opposite directions. 

Given a loss function, we can calculate the loss gradient $\nabla_\theta L$ with respect to each of the weights and biases in the model. This gradient expresses how each parameter is connected to the loss. The overall idea is then to  ``nudge'' each parameter in the right direction. We generally denote a full cycle of forward and backpropagation and an update to the parameters as an epoch. We calculate the updated parameter $\theta_t$ for epoch $t$ using gradient descent 
\begin{align*}
  \theta_{t} = \theta_{t-1} - \eta \nabla_\theta L(\theta_t).
\end{align*}
Gradient descent is analog to taking a step in parameter space in the direction
that yields the biggest decrease in the loss. If we imagine a simplified analog
with only two parameters $\theta_1$ and $\theta_2$ we can think of this as the
act of stepping perpendicular to the contour lines shaped by the loss function
as shown in \cref{fig:gd}. Notice however that models in general contain on the order of $\num{e6}--\num{e9}$ parameters \hl{FACT CHECK}, but this might get a bit harder to visualize. The length of each step is proportional to the gradient magnitude and the learning rate $\eta$. There are three main flavors to the gradient descent: Batch,
stochastic and mini-batch gradient descent. In batch gradient descent we simply
calculate the gradient based on the whole dataset by averaging the contribution
from each data point before updating the parameters. This gives the most robust estimate of the gradient and thus the most direct path through parameter space in terms of minimizing the loss function as
indicated in \cref{fig:gd}. However, for big datasets, this calculation can be
computationally heavy as it must carry the entire dataset in memory at once. A
solution to this issue is provided by stochastic gradient descent
(\acrshort{SGD}) which considers only one data point at a time. Each data point is chosen randomly and the parameters are updated based on the corresponding gradient. This leads to more frequent updates of the parameters which will results
in a more ``noisy'' path through parameter space with respect to minimizing the
loss as shown in \cref{fig:sgd}. In some situations, this might compromise the
precision but the noisiness makes it more likely to escape local minima in the loss space. The mini-batch gradient descent serves as a middle ground between the above methods by dividing the full dataset into a subset of mini-batches. Each parameter update is then based on the gradient within a mini-batch. By choosing a suitable batch size we get the robustness of the (full) batch gradient descent and the computational efficiency and resistance to local minima of the \acrshort{SGD} method. 


\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/gd.png}
    \caption{}
    \label{fig:gd}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/sgd.png}
    \caption{}
    \label{fig:sgd}
  \end{subfigure}
  \hfill
  % https://www.samlau.me/test-textbook/ch/11/gradient_stochastic.html
  \caption{\hl{TMP}}
  \label{fig:gradient_descent}
\end{figure}



\section{Optimizers}
% Consider changing from theta_t+1 to theta
% https://arxiv.org/pdf/1412.6980.pdf

The name \textit{optimizers} covers a variety of gradient descent methods. In our study, we will use the ADAM (adaptive moment estimation). ADAM combines several ``tricks in the book'' which we will introduce in the following.

% Momentum 
One considerable extension of the gradient descent scheme is by the introduction of a momentum term $m_t$ such that we get
\begin{align}
  \theta_t = \theta_{t-1} - m_t, \qquad m_t = \alpha m_{t-1} + \eta \nabla_\theta L(\theta_t)
  \label{eq:mom}
\end{align}
with $m_0 = 0$. If we introduce the shorthand $g_t = \nabla_\theta L(\theta_t)$ we find
\begin{align}
  m_1 &= \alpha m_0 + \eta g_1 = \eta g_1 \nonumber \\
  m_2 &= \alpha m_1 + \eta g_2 = \alpha^1 \eta g_1 + \eta g_2 \nonumber \\
  m_3 &= \alpha m_2 + \eta g_3 = \alpha^2 \eta g_1 + \alpha\eta g_2 + \eta g_3 \nonumber \\
  &\vdots \nonumber \\
  m_t &= \eta \left(\sum_{k=1}^{t} \alpha^{t-k}g_k\right).
  \label{eq:mom_rec}
\end{align}
Hence $m_t$ is a weighted average of the gradients with an exponentially decreasing weight. This act as a memory of the previous gradients and aid to pass local minima and to some degree plateaus in the loss space. A variation of momentum can be achieved with the introduction of the exponential moving average (EMA) which builds on the recursion

\begin{align*}
    \text{EMA}(g_1) &= \alpha \overbrace{\text{EMA}(g_0)}^{\equiv \ 0} + (1-\alpha)g_1 \\
    \text{EMA}(g_2) &= \alpha \text{EMA}(g_1) + (1-\alpha)g_2 \\
    &\vdots \\
    \text{EMA}(g_t) &= \alpha \text{EMA}(g_{t-1}) + (1-\alpha)g_t  = \sum_{k=0}^t \alpha^{t-k}(1-\alpha)g_t,
\end{align*}
which is similar to that of momentum \cref{eq:mom_rec}, but with the explicit weighting by $(1-\alpha)$.

The second moment of the exponential moving average is utilized in the root mean square propagation (\acrshort{RMSProp}) method which is motivated by the issue of passing long plateaus in the loss space. Since the size of the updates are proportional to the norm of the gradient
\begin{align*}
  \theta_{t+1} = \theta_t - \eta g_t \ \Longrightarrow \ ||\theta_{t+1}-\theta_{t}|| = \eta ||g_t||,
\end{align*}
we might get the idea of normalizing the gradient step by dividing with the norm $|||g_t|$.  However, this does not immediately solve the problem of long plateaus as we need to consider multiple past gradients which is then accommodated by the use of the \acrshort{EMA}. When reentering a steep region again we need to ``quickly'' downscale the gradient steps again which can be achieved by using the squared norm $||g_t||^2$ for the \acrshort{EMA} which makes it more sensitive to outliers. The \acrshort{RMSProp} update scheme then becomes
\begin{align}
  \theta_t = \theta_{t-1} - \eta \frac{g_t}{\sqrt{\text{EMA}(||g_t||^2)}+ \epsilon},
  \label{eq:RMSProp}
\end{align}
where $\epsilon$ is simply a small number to avoid division by zero issues. 

ADAM merges the idea of first order \acrshort{EMA} for the momentum $m_t$, and the second order \acrshort{EMA} $v_t$, as used in the root mean square propagation technique in \cref{eq:RMSProp}
\begin{align*}
  m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t, \\
  v_t &= \beta_2 v_{t-1} + (1-\beta_2)g_t^2. 
\end{align*}
Since these are initially set to zero we can correct a bias towards zero by a scaling term $(1-\beta^t_1)$ and $(1-\beta^t_2)$ respectively such that the ADAM scheme becomes
\begin{align}
  \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \qquad \qquad \hat{m}_t = \frac{m_t}{1-\beta^t_1}, \qquad \hat{v}_t = \frac{v_t}{1-\beta^t_2}.
  \label{eq:ADAM}
\end{align}


\subsection{Weight decay}
By adding a so-called \textit{regularization} to the loss function we can penalize high magnitudes of the model parameters. This is motivated by the idea of preventing overfitting during training. The most common way to do this is by the use of L2 regularization, adding the squared $l^2$ norm $||\theta||_2^2$, where $||\theta||_2 = \sqrt{\theta_1^2 + \theta_2^2 + \ldots}$, to the model. The new loss and gradient then become
\begin{align}
  L_{l^2}(\theta) &= L(\theta) + \frac{1}{2}\lambda ||\theta||^2_2 \nonumber \\
  \nabla_\theta L_{l2}(\theta) &=  \nabla_\theta L(\theta) + \lambda \theta,
  \label{eq:l2_grad}
\end{align}
where $\lambda$ is the weight decay parameter $\in [0,1]$. The name \textit{weight decay} relates to the fact that some practitioners only apply this penalty to the weights in the model, but we will include the biases as well (standard in PyTorch). Following the original gradient descent scheme \cref{eq:l2_grad} we get
\begin{align*}
  \theta_{t+1} &= \theta_t - \eta g_t - \eta\lambda \theta_t \\
  &= \theta_t\underbrace{(1-\eta\lambda)}_{\text{Weight decay}} - \eta g_t.
\end{align*}
Thus we notice that choosing a high weight decay (towards 1) will downscale the model parameters while choosing a low weight decay (towards 0) yields the original gradient descent scheme. Note that this is used in combination with ADAM, but it is easier to show the consequences for the original gradient descent scheme.  



\subsection{Parameter distributions}
In order to get optimal training conditions it has been found that the initial
state of the weight and biases are important \hl{SOURCE}. First of all, we must
initialize the weight by sampling from some distribution. If the weights are set
to equal values the gradient across a layer would be the same. This results in a
complexity reduction as the model can only encode the same values across the
layer \hl{SOURCE}. Further, we want to consider the gradient flow during training. Especially for deep networks, networks with many layers, we must pay attention to the problem of vanishing or exploding gradients. If we for instance consider the sigmoid activation function and its derivative 
\begin{align*}
  f(z) = \frac{1}{1 + e^{-z}}, \qquad f'(z) = \frac{df(z)}{dz} = \frac{e^{-z}}{(1+e^{-z})^2} = \frac{e^{z}}{(1+e^{z})^2},
\end{align*}
we notice that for large and small values we get $f(z\to \pm\infty) \to 0$.
However, even a small gradient will vanish throughout a deep network as the
calculation of the gradient involves the chain rule. A similar problem can be
found with the ReLU activation function which gets a zero gradient for inputs
$z<0$ which can be mitigated by the so-called leaky RelU which gives the $z<0$ a
small slope. On the other hand, we have exploding gradients, which are simply a
result of the chain rule gradient calculation. For a sufficiently deep network,
the gradient can grow exponentially and sometimes result in numerical overflow.
While there exist techniques to accommodate this, like for instance the leaky
ReLU for the vanishing gradients and so-called gradient clipping, cutting off
the gradient at a maximum, they both benefit from a properly initialized set of
weights \hl{SOURCE}. That is, we want the gradients across a given layer to have
a zero mean while the variance is similar to other layers. This balanced
gradient flow is more likely to happen if we initialize the weight by the same
set of criteria. The specific actions to achieve this depend on model architecture, including the choice of activation functions. For instance, using the ReLU activation functions it was found that the single standard deviation will depend on the number of input nodes from the previous layer $N^{[l-1]}$ as $\sim \sqrt{N^{[l-1]}}/\sqrt{2}$ and thus we simply scale a zero mean uniform distribution to match this.  This is part of the Kaiming initialization scheme which is standard in Pytorch \hl{SOURCe}. \hl{Mention choices for bias initialization}.


Batch normalization is another technique that might also help reduce the issue of poor gradient flow. Furthermore, can benefit by speeding up convergence making it more stable \hl{SOURCE}. In general, model parameters are modified throughout training meaning that the range of values coming from a previous layer will shift, even though the same training data is fed through the network repeatedly.  By scaling the input for a given layer, for each mini-batch, we can mitigate this problem and make for a more standardized input range. This often result in a faster training convergence. For layer $l$ we calculate the mean $\mu^{[l]}$ and variance $\sigma^{2[l]}$ across the layer with nodes $x_1^{[l]}, x_2^{[l]}, \ldots, x_d^{[l]}$ for each mini-batch of size $m$ as
\begin{align*}
  \mu^{[l]} = \frac{1}{m} \sum_i z^i, \qquad \sigma^{2[l]} = \frac{1}{m} \sum_i^d (x^i-\mu)^2.
\end{align*}
We then perform normal scaling of the inputs within the batch
\begin{align*}
  \hat{x}_i^{[l]} = \frac{x_i^{[l]} - \mu^{[l]}}{\sqrt{\sigma^{2[l]} + \epsilon}},
\end{align*}
where $\epsilon$ is a small number to ensure numerical stability (similar to what we used for RMSProp gradient descent). In the final step the input values are rescaled as
\begin{align*}
  \tilde{x_i} = \gamma^{[l]} \hat{x}_i^{[l]} + \beta^{[l]}
\end{align*}
with trainable parameters $\gamma$ and $\beta$. \hl{Comment about the reason for the final step}.





\subsection{Learning rate decay strategies}

\section{Convolutional Neural Network}
Convolutional Neural Network (\acrshort{CNN}) builds on feed forward neural
network, but is specialized for spatial inputs such as images. The neural
network applies a so-called \textit{kernel} which is a filter sliding over the
image. for a multi-channel image, such as the standard RGB containing a channel
for red, green and blue colors, the kernel must have a depth that matches this.
The input is processed as a dot product between similar channels in the input
and the kernel for which the resulting values are summed and added with a single
bias set for the whole kernel as illustrated in \cref{fig:conv_example}. Each
kernel represents as a set of trainable weights and a bias. Often multiple
kernels is applied to the input which results in an increasing number of
channels throughout the model. On the otherside the spatial input is
down-scaled. By controlling the \textit{stride}, the translation of the filter
between each convolution, we can alter the amount of down-scaling. If we keep
the stride and apply a paddin around the input, using zeroes or extending the
edge for instance, we can also choose to preserve the spatial size in the
convolution. The spatial size is given by the formula 
\begin{align}
  N_{i+1} = \left\lfloor \frac{N_i - F + 2P}{S} + 1 \right\rfloor,
  \label{eq:down_scaling}
\end{align}
for padding $P$, stride $S$, spatial size of the kernel filter $F$, spatial size
of input $N_i$ and spatial size of output $N_{i+1}$. The down-samling feature is
often ... through a pooling layer. A pooling layer process the input by taking
the mean (mean pooling) or the max value (max pooling) in the filter as it
slides over the input. This is often used after a convolution meaning that the
input is given as the resulting feature map. For instance, by making a max
pooling of size $2 \times 2$ by stride two we essentially half the dimensions of
the image, but this follows \cref{eq:down_scaling}. 


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/CNN.png}
  \caption{\hl{TMP}}
  % https://www.researchgate.net/figure/Representation-of-a-Convolutional-Neural-Network-The-CNN-performs-automatic-spatial_fig3_359896672
  \label{fig:CNN}
\end{figure}


\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.26\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/theory/conv_kernel_movement.png}
      % \includegraphics[width=\textwidth]{figures/theory/conv_input_channels.png}
      \caption{}
      % \label{fig:}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.70\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/theory/conv_calculation.png}
      \caption{}
      % \label{fig:}
  \end{subfigure}
  \hfill
  % https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53
  \caption{\hl{TMP}}
  \label{fig:conv_example}
\end{figure}

For a convolution process, we often consider the \textit{receptive field}. The
receptive field relates to the spatial size of the input that affects a given
node in the feature map at a given layer of the model. In
\cref{fig:receptive_field} the receptive field is illustrated of a 1D
representation of convolutional with a filter of stride 1 and filter width 2.
Going from the output and backwards, we see that the output relates to two nodes
in the previous last hidden layer. Each of these nodes is connected to two nodes
in the layer before that, however with on of them being the same due to the
stride of 1. By back tracking all the way to the input we see that this
corresponds to a receptive field of $D = 5$. By increasing the filter size and
the stride the 2D receptive field will grow a lot faster than shown in this
simple 1D example. For a receptive field $D$, in a given spatial dimension, a
spatial size of the filter $F$, stride $S_i$ (from layer $i$ to $i-1$), layer
index: $l = \{1,2,3,\cdots,n\}$, we have
\begin{align*}
    D_l = D_{l-1} + \left[(F_l - 1) \cdot \prod_{i=1}^{l-1}S_i \right],
\end{align*}
with $D_0 = 1$ and $l=0$ as the input layer. Note that by convention, the
product of 0 is 1, so for the first layer, the sum over $S$ is 1. The receptive
field is important in understanding the connectivity in the model. The model
output will be completely independent of the inputs and feature maps outside the
receptive field. Furthermore, since nodes in the center of the receptive field
cones often have multiple connective paths leading to the output, as seen in
\cref{fig:receptive_field} as well, we differentiate between the theoretical
receptive field and the effective receptive field. The effective receptive field
will have a Gaussian distribution within the theoretical receptive field due to
the fact that the nodes in the center will be more represented as multiple paths
connect to the output. Thus we in practice consider the effective receptive
field to be smaller than the theoretical. Implementations like dilated
convolutions, which make the filter expand in circumforence and skipping
skipping positions within the filter, can be used to further increase the
effective field. 

The power of the \acrshort{CNN} lies mainly in the ability to carry
translational invariance as opposed to a dense neural network. If practice, if
you train a dense network to classify cat images and only show images with cats
on the left side it will fail horribly when predicting on similar imiages with
cats on the right side of the image. The \acrshort{CNN} is less proven to such
issues as each kernel translates the whole image using the same weights. This is
also the reason that the \acrshort{CNN} architecture is more parameter
efficient. For a dense network it would essentially need to have
``cat-detecting'' weights and bias configurations for all parts of the input.
This forces a dense network to learn similar trends for multiple part of the
network in contrast to the \acrshort{CNN}.



\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/receptive_field.png}
  \caption{\hl{TMP}}
  % https://lukeguerdan.com/blog/2019/intro-to-tcns/
  \label{fig:receptive_field}
\end{figure}

\subsection{Training, validation and test data}
So far, we have simply considered the concept of training data as a means to
update the model parameters. Yet, we want to evaluate the model performance is
it improves. If a model has enough complexity, i.e.\ enough layers with a
sufficient amount of nodes, it can be proven that it can fit any function
\hl{SOURCE}. Thus for a complex model, it is just a matter of time before the
model eventually will learn to give matching predictions for the training data.
However, we want the model to learn general trends and not to ``memorize'' all
the data points which is known as overfitting. While the predictions can get
arbitrarily good the performance on unseen data within the same task will yield
awful results if the model is overfitted. Hence, we put aside some subset of the
data which we use as a \textit{validation} throughout and after training. By
keeping this \textit{validation} set separate from the training data we can get
a more reliable performance estimate for the model. This splitting of data
should be done randomly in order to ensure a similar distribution of data
features in each set. For the size ratio we want to strike a balance between
training quality and validation quality, usually a 20\%--80\% split favoring the
training set is a good figure. Other techniques exist which aim to optimize the
data use for sparse data situations, like cross-validation and bootstrap
\hl{fact check}, but we will not consider such methods for this thesis. A third
training set which is often forgoten is the \textit{test} set. While the
validation set should be kept unseen from the model training, the test set
should be kept unseen from the model developer. As we choose the model
architecture and hyper-parameters like learning rate, momentum and weight decay,
we will use the performance on the validation set as a metric to steer by. This
represents a slower pace parameter fitting which can also led to overfitting on
a higher level. Hence, we should denote a test set for the final evaluation of
our model which have not be considered before the end. Formally, this is the only reliable performance metric for the model. 



\section{Overfitting and underfitting}
% THE UNDERFITTING AND OVERFITTING TRADE-OFF \cite{smith2018disciplined}. 

% learning rate under and over fitting
% https://www.javatpoint.com/gradient-descent-in-machine-learning


% Underfitting
Underfitting is recognized as a loss curve that continues to decrease slowly. This is highly dependent on the complexity of the model and the learning rate. A to small learning rate will induce underfitting

% Overfitting
Overfitting a more complicated to diagnostic. The Dynamics is not as simple as sketched in \cref{fig:over_under_fitting}.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/over_under_fitting.png}
  \caption{\hl{TMP} \cite{smith2018disciplined}}
  \label{fig:over_under_fitting}
\end{figure}

% \section{More stuff}
% \begin{enumerate}
%   \item What about performance measuring?
% \end{enumerate}



\section{Hypertuning}
Training a machine learning model revolves around tuning the model parameters
consisting of mainly weights and biases. By following the general idea by the
gradient descent approach we can get complex models able to give complex
predictions. However, a handful of so-called \textit{hyper-parameters} remain
for us to decide. First of all we must decide on an architecture for the model.
This includes high level considerations as for instance to use neural network or
a convolutional network, but also lower level considerations like the depth of
the model and number of nodes/channels per layer. The choice of loss function
and optimizers come with hyperparameters like learning rate, momentum and weight
decay. As N. Smith \cite{smith2018disciplined} puts it: ``Setting the
hyper-parameters remains a black art that requires years of experience to
acquire''. In the following, we will review some general approaches to the
hypertuning presented in \cite{smith2018disciplined}. The traditional way is to
perform a grid-search of multiple training sessions with different combinations
of hyper-parameters. This can be computationally expensive. In addition, hyper-parameters will depend on each other, the data and the architecture which can realistically be searched at the same time. 

N. Smith points to the fact that validation loss can by examined early on for
clues of either underfitting or overfitting. For the learning rates one can use
an increasing learning rate scheme for the so-called \textit{learning rate range
test} (LR range test). One specifies a minimum and a maximum learning rate
boundaries and a learning rate step size.  This can be done both for linear
increasing or decreasing learning rate, which proved to give similar results
\cite{smith2018disciplined} \hl{Jastrzebski et al. (2017b)}. We choose the
linear increasing version for simplicity. In the LR range test training then
starts form the lowest learning rate which is increased in small steps during a
pre-training. The stepsize can be number of mini-batch iterations, and does not
have to stick to epochs. For small learning rates the network will converge
slowly. As the learning rate apporahces an appropriate size the convergence more
strongly. Eventually the convergence will stop and the loss will increase again
as the learning rates becomes to high (\hl{show harmonic potential big learning
rate example somewhere?}). The learing rates at the point of divergence indicate an upper bound the learning rate that can be used with a cyclic learing rate scheme. One have to set the upper bound a bit lower than this to avoid divergence in practice. The steepest decline of the validation curve can be used as an estimate for the best constant learning rate choice. 

Next up, is momentum. Momentum and learning rates effect each other and since
the learning rate is regarded as one of the most important hyper-parameters to
tune \hl{SOURCE} momentum is also important. We want to set both learning rate
and momentum as large as possible without causing instabilities to the training.
Momentum is designed to accelerate the training process but the effect on
updating the weights is of the same magnitude as learning rate as present from
\cref{eq:mom}. Experiments show that finding a the right momentum from momentum
range test is not useful. Thus, short runs with different values of momentum, on
the range 0.9--0.99, can be done to find the best choice of momentum. The
learning rate and momenmtum will however often be inversely dependent. Choosing
a high learning rate will result in a slightly lower momentum being appropriate
and vice versa. Thus, when using a cycling learning rate scheme one should adopt
a cyclic momentum scheme as well with decreasing momentum for increasing
learning rate. Choosing a lower momentum of 0.80--0.85 often gives similar stabile results.

Finally we address the weight decay. N. Smith reports that weight decay is not similar to learning rate and momentum as it is better to choose a constant weight decay through the training. This will be dependent on the model complexity and he suggest doing a grid-search for values such as 0, $\num{e-6}$, $\num{e-5}$ and $\num{e-4}$ for complex architecture and $\num{e-4}$, $\num{e-3}$ and $\num{e-2}$ for more shallow architectures. Choosing the weight decay on the scale of exponential exponents is often good enough as the sensitivity to precise factors is not found to be highly important. The optimal weight decay will depend on your learning rate and momentum scheme.






% Since learning rate is regarded as the most important hyper-parameter to tune (Bengio, 2012) \cite{smith2018disciplined}







\section{Prediction explanation}
Looking at feature maps and gradient maps.


% \begin{itemize}
%   \item Feed forward fully connected
%   \item CNN
%   \item GAN (encoder + decoder)
%   \item Genetic algorithm
%   \item Using machine learning for inverse designs partly eliminate the black
%   box problem. When a design is produced we can test it, and if it works we not
%   rely on machine learning connections to verify it's relevance. 
%   \item However, using explanaitons techniques such as maybe t-SNE, Deep dream,
%   LRP, Shapley values and linearizations, we can try to understand why the AI
%   chose as it did. This can lead to an increased understanding of each design
%   feature. Again this is not dependent on the complex network of the network as
%   this can be tested and veriied independently of the network. 
% \end{itemize}

% \section{Feed forward network / Neural networks}
% \section{CNN for image recognition}
% \section{GAN (encoder + deoder)}
% \section{Inverse desing using machine learning}
% \section{Prediction explanation}
% \subsection{Shapley}
% \subsection{Lineariations}
% \subsection{LRP}
% \subsection{t-SNE}

\section{Accelerated search using genetic algorithm}


\subsection{Markov-Chain Accelerated Genetic Algorithms}
% Following the article:
% Accelerated Genetic Algorithms with Markov Chains
% Guan Wang, Chen Chen, and K.Y. Szeto

\subsubsection{Talk about traditional method also?}

\subsubsection{Implementing for 1D chromosone (following article closely)}

We have the binary population matrix $A(t)$ at time (generation) $t$ consisting of $N$ rows denoting chromosones and with $L$ columns denoting the so-called locus (fixed position on a chromosome where a particular gene or genetic marker is located, wiki). We sort the matrix rowwise by the fitness of each chromosono evaluated by a fitness function $f$ such that $f_i(t) \le f_k(t)$ for $i \ge k$. We assume that there are a transistion probability between the current state $A(t)$ and the next state $A(t+1)$. We consider this transistion probability only to take into account mutation process (mutation only updating scheme). During each generation chromosones are sorted from most to least fitted. The chromosone at the $i$-th fitted place is assigned a row mutation probability $a_i(t)$ by some monotonic increasing function. This is taken to be 
\begin{align*}
  a_i(t) = 
  \begin{cases}
    (i-1)/N',& i-1 < N' \\
    1, &\text{else}
  \end{cases}
\end{align*}
for some limit $N'$ (refer to first part of article talking about this). We use $N' = N/2$. We also defines the survival probability $s_i = 1 - a_i$. In thus wau $a_i$ and $s_i$ decide together whether to mutate to the other state (flip binary) or to remain in the current state. We use $s_i$ as the statistical weight for the $i$-th chromosone given it a weight $w_i = s_i$.
\\
Now the column mutation. For each locus $j$ we define the count of 0's and 1's as $C_0(j)$ and $C_1(j)$ resepctively. These are normalized as
\begin{align*}
  n_0(j, t) = \frac{C_0(j)}{C_0(j) + C_1(j)}, \quad n_1(j, t) = \frac{C_1(j)}{C_0(j) + C_1(j)}.
\end{align*}
These are gathered into the vector $\vec{n}(j,t)=(n_0(j, t), n_1(j, t))$ which characterizes the state distribution of $j$-th locus. In order to direct the current population to a preferred state for locus $j$ we look at the highest weight of row $i$ for locus $j$ taking the value 0 and 1 respectively.
\begin{align*}
  C'_0(j) &= \max\{W_i | A_{ij} = 0; \ i = 1, \cdots, N\} \\
  C'_1(j) &= \max\{W_i | A_{ij} = 1; \ i = 1, \cdots, N\}
\end{align*}
which is normalized again
\begin{align*}
  n_0(j, t+1) = \frac{C'_0(j)}{C'_0(j) + C'_1(j)}, \quad n_1(j, t+1) = \frac{C'_1(j)}{C'_0(j) + C'_1(j)}.
\end{align*}
The vector $\vec{n}(j,t+1)=(n_0(j, t+1), n_1(j, t+1))$ then provides a direction for the population to evolve against. This characterizes the target state distribution of the locus $j$ among all the chromosones in the next generation. We have
\begin{align*}
  \begin{bmatrix}
    n_0(j, t+1) \\
    n_1(j, t+1)
  \end{bmatrix}
  = 
  \begin{bmatrix}
    P_{00}(j,t) \ P_{10}(j,t) \\
    P_{01}(j,t) \ P_{11}(j,t)
  \end{bmatrix}
  \begin{bmatrix}
    n_0(j, t) \\
    n_1(j, t)
  \end{bmatrix}
\end{align*}
Since the probability must sum to one for the rows in the P-matrix we have 
\begin{align*}
  P_{00}(j, t) = 1 - P_{01}(j, t), \quad P_{11}(j, t) = 1 - P_{10}(j, t)
\end{align*}
These conditions allow us to solve for the transition probability $P_{10}(j,t)$ in terms of the single variable $P_{00}{j,t}$.
\begin{align*}
  P_{10}(j,t) &= \frac{n_0(j, t+1) - P_{00}(j,t)n_0(j, t)}{n_1(j,t)} \\
  P_{01}(j,t) &= 1 - P_{00}(j,t) \\
  P_{11}(j,t) &= 1 - P_{10}(j,t)
\end{align*}
We just need to know $P_{00}(j,t)$. We start from $P_{00}(j, t = 0) = 0.5$ and then choose $P_{00}(j,t) = n_0(j,t)$




\subsubsection{Repair function}
Talk about it here or in random walk section?