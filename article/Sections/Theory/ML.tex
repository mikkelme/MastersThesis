\chapter{Machine Learning}
In this thesis machine learning will serve as a numerical tool for evaluating
and exploring the frictional behavior of various Kirigami designs. We will
generate data using \acrshort{MD} simulations which serve as a ground truth for
the training of a machine learning model. If successful, we can utilize such a
model to predict the frictional behavior of unseen configurations. The machine
learning predictions will be a lot faster than carrying out a complete
\acrshort{MD} simulation and thus this can be used to accelerate the search
through a new set of configurations. It is not obvious that the machine learning
model can readily capture the physical mechanisms in our system. Hence, the
attempt to model the system with machine learning also has value in terms of
revealing the usefulness of such methods to this problem. We aim to implement a
rather traditional machine-learning approach. In this chapter we introduce the
key concept behind machine learning and some of the relevant concepts and
techniques relevant to our implementation.


\section{Neural network}
The neural network, or more precisely the \textit{feed forward dense neural
network}, is one of the original concepts in machine learning arising from the attempt of mimicking the way neurons work in the brain \cite{lederer2021activation, Shankar_2022}. The neural network can be considered as three major parts: The input layer, the
so-called \textit{hidden layers} and finally the output layer as shown in
\cref{fig:ffnn}. The input is described as a vector $\vec{x} = x_0, x_1, \ldots,
x_{n_x}$ where each input $x_i$ is usually denoted as a \textit{feature}. For
our task we will consider the Kirigami configuration, load and stretch of the system as input features on which we want the model to base its prediction. The
input features are densely connected to each of the \textit{nodes} in the first
hidden layers as indicated by straight lines in \cref{fig:ffnn}. Each line
represents a weighted connection that can be adjusted to configure the
importance of that feature. Similar dense connections run through all the
hidden layers to the final output layer. For a given note $a_j^{[l]}$ in layer $l$ will process the input from layer $l-1$ as
\begin{align*}
  a_j^{[l]} = f\left(\sum_i w^{[l]}_{ij}a_i^{[l-1]} + b_j^{[l]}\right),
\end{align*}
where $w^{[l]}_{ij}$ is the weight connection node $a_i^{[l-1]}$ of the previous layer to the node $a_j^{[l]}$ in the current layer. Note that the choice of denoting this weight to belong to layer $l$ as opposed $l-1$ is simply a notation choice. $b_j^{[l]}$ denotes a bias and $f(\cdot)$ the \textit{activation function}. The activation function is often chosen to give a non-linear mapping of the input to each node. Without this, the network will only be capable of approximate linear functions \cite{lederer2021activation}. Two common activation functions are the \textit{sigmoid}, mapping the input to the range $(0,1)$, and the ReLU which cuts off negative values and maps positive linearly
\begin{align*}
  \text{Sigmoid:} \quad f(z) = \frac{1}{1 + e^{-z}}, \qquad \qquad
  \text{ReLU:} \quad 
  f(z)= \begin{cases}
    z \text{ for } z > 0   \\
    0 \text{ for } z \leq 0.
    \end{cases}
\end{align*}
Often the same activation function is used throughout the network, except for the output layer where the activation function is usually omitted or the sigmoid is used for classification tasks. The whole process of sending data through the model is called \textit{forward propagation} and constitutes the mechanism for mapping an input $\vec{x}$ to the model output $\hat{vec{y}}$. In order to get useful predictions we must \textit{train} the model which involves tuning the model parameters, the weight and biasses.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/ffnn.png}
  \caption{\hl{From overleaf IN400}}
  \label{fig:ffnn}
\end{figure}


The model training relies on two core concepts: \textit{backpropagation} and \textit{gradient descent} optimization. We define the error associated with a model prediction, otherwise known as the \textit{loss}, through the \textit{loss function} $L$ that evaluates the model output  $\hat{\vec{y}}$ against the true value $\vec{y}$. For a continuous scalar output, we might simply use the mean squared error (\acrshort{MSE})
\begin{align*}
  L_{\text{\acrshort{MSE}}} = \frac{1}{N_y} \sum_{i = 1}^N (y_i - \hat{y}_i)^2.
\end{align*}
For a binary classification problem, meaning that the true output is True or False (1 or 0), a common choice is binary cross entropy (BSE)
\begin{align*}
  L_{\text{BSE}} =  -\sum_{i=1}^n \ \Big[y_i\log(\hat{y_i}) + (1-y_i)\log(1 - \hat{y_i}) \Big] =  \sum_{i=1}^n   \begin{cases}
    - \log{(\hat{y_i})},& \quad y_i = 1 \\
    -\log{(1-\hat{y_i})},& \quad y_i = 0.
\end{cases}
\end{align*}
The cross-entropy loss can be derived from a maximum likelihood estimation \hl{SOURCE}. Without going into details with the derivation we can convince ourselves that the error is minimized for the correct prediction and maximized for the worst prediction. When $y_i = 1$ we get the negative term $-\log(\hat{y_i})$ where a correct prediction $\hat{y}_i \to 1$ yields a loss contribution $L_i \to 0$. For a wrong prediction $\hat{y}_i \to 0$ the loss contribution will diverge $L_i \to \infty$. Similar applies to the case of $y_i = 0$ with opposite directions. 

Given a loss function, we can calculate the loss gradient $\nabla_\theta L$ with respect to each of the weights and biases in the model. This gradient expresses how each parameter is connected to the loss. The overall idea is then to  ``nudge'' each parameter in the right direction. We generally denote a full cycle of forward and backpropagation and an update to the parameters as an epoch. We calculate the updated parameter $\theta_t$ for epoch $t$ using gradient descent 
\begin{align*}
  \theta_{t} = \theta_{t-1} - \eta \nabla_\theta L(\theta_t).
\end{align*}
Gradient descent is analog to taking a step in parameter space in the direction
that yields the biggest decrease in the loss. If we imagine a simplified analog
with only two parameters $\theta_1$ and $\theta_2$ we can think of this as the
act of stepping perpendicular to the contour lines shaped by the loss function
as shown in \cref{fig:gd}. Notice however that models in general contain on the order of $\num{e6}--\num{e9}$ parameters \hl{FACT CHECK}, but this might get a bit harder to visualize. The length of each step is proportional to the gradient magnitude and the learning rate $\eta$. There are three main flavors to the gradient descent: Batch,
stochastic and mini-batch gradient descent. In batch gradient descent we simply
calculate the gradient based on the whole dataset by averaging the contribution
from each data point before updating the parameters. This gives the most robust estimate of the gradient and thus the most direct path through parameter space in terms of minimizing the loss function as
indicated in \cref{fig:gd}. However, for big datasets, this calculation can be
computationally heavy as it must carry the entire dataset in memory at once. A
solution to this issue is provided by stochastic gradient descent
(\acrshort{SGD}) which considers only one data point at a time. Each data point is chosen randomly and the parameters are updated based on the corresponding gradient. This leads to more frequent updates of the parameters which will results
in a more ``noisy'' path through parameter space with respect to minimizing the
loss as shown in \cref{fig:sgd}. In some situations, this might compromise the
precision but the noisiness makes it more likely to escape local minima in the loss space. The mini-batch gradient descent serves as a middle ground between the above methods by dividing the full dataset into a subset of mini-batches. Each parameter update is then based on the gradient within a mini-batch. By choosing a suitable batch size we get the robustness of the (full) batch gradient descent and the computational efficiency and resistance to local minima of the \acrshort{SGD} method. 


\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/gd.png}
    \caption{}
    \label{fig:gd}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/sgd.png}
    \caption{}
    \label{fig:sgd}
  \end{subfigure}
  \hfill
  % https://www.samlau.me/test-textbook/ch/11/gradient_stochastic.html
  \caption{\hl{TMP}}
  \label{fig:gradient_descent}
\end{figure}



\section{Optimizers}
% Consider changing from theta_t+1 to theta
% https://arxiv.org/pdf/1412.6980.pdf

The name \textit{optimizers} covers a variety of gradient descent methods. In our study, we will use the ADAM (adaptive moment estimation). ADAM combines several ``tricks in the book'' which we will introduce in the following.

% Momentum 
One considerable extension of the gradient descent scheme is by the introduction of a momentum term $m_t$ such that we get
\begin{align}
  \theta_t = \theta_{t-1} - m_t, \qquad m_t = \alpha m_{t-1} + \eta \nabla_\theta L(\theta_t)
  \label{eq:mom}
\end{align}
with $m_0 = 0$. If we introduce the shorthand $g_t = \nabla_\theta L(\theta_t)$ we find
\begin{align}
  m_1 &= \alpha m_0 + \eta g_1 = \eta g_1 \nonumber \\
  m_2 &= \alpha m_1 + \eta g_2 = \alpha^1 \eta g_1 + \eta g_2 \nonumber \\
  m_3 &= \alpha m_2 + \eta g_3 = \alpha^2 \eta g_1 + \alpha\eta g_2 + \eta g_3 \nonumber \\
  &\vdots \nonumber \\
  m_t &= \eta \left(\sum_{k=1}^{t} \alpha^{t-k}g_k\right).
  \label{eq:mom_rec}
\end{align}
Hence $m_t$ is a weighted average of the gradients with an exponentially decreasing weight. This act as a memory of the previous gradients and aid to pass local minima and to some degree plateaus in the loss space. A variation of momentum can be achieved with the introduction of the exponential moving average (EMA) which builds on the recursion

\begin{align*}
    \text{EMA}(g_1) &= \alpha \overbrace{\text{EMA}(g_0)}^{\equiv \ 0} + (1-\alpha)g_1 \\
    \text{EMA}(g_2) &= \alpha \text{EMA}(g_1) + (1-\alpha)g_2 \\
    &\vdots \\
    \text{EMA}(g_t) &= \alpha \text{EMA}(g_{t-1}) + (1-\alpha)g_t  = \sum_{k=0}^t \alpha^{t-k}(1-\alpha)g_t,
\end{align*}
which is similar to that of momentum \cref{eq:mom_rec}, but with the explicit weighting by $(1-\alpha)$.

The second moment of the exponential moving average is utilized in the root mean square propagation (\acrshort{RMSProp}) method which is motivated by the issue of passing long plateaus in the loss space. Since the size of the updates are proportional to the norm of the gradient
\begin{align*}
  \theta_{t+1} = \theta_t - \eta g_t \ \Longrightarrow \ ||\theta_{t+1}-\theta_{t}|| = \eta ||g_t||,
\end{align*}
we might get the idea of normalizing the gradient step by dividing with the norm $|||g_t|$.  However, this does not immediately solve the problem of long plateaus as we need to consider multiple past gradients which is then accommodated by the use of the \acrshort{EMA}. When reentering a steep region again we need to ``quickly'' downscale the gradient steps again which can be achieved by using the squared norm $||g_t||^2$ for the \acrshort{EMA} which makes it more sensitive to outliers. The \acrshort{RMSProp} update scheme then becomes
\begin{align}
  \theta_t = \theta_{t-1} - \eta \frac{g_t}{\sqrt{\text{EMA}(||g_t||^2)}+ \epsilon},
  \label{eq:RMSProp}
\end{align}
where $\epsilon$ is simply a small number to avoid division by zero issues. 

ADAM merges the idea of first order \acrshort{EMA} for the momentum $m_t$, and the second order \acrshort{EMA} $v_t$, as used in the root mean square propagation technique in \cref{eq:RMSProp}
\begin{align*}
  m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t, \\
  v_t &= \beta_2 v_{t-1} + (1-\beta_2)g_t^2. 
\end{align*}
Since these are initially set to zero we can correct a bias towards zero by a scaling term $(1-\beta^t_1)$ and $(1-\beta^t_2)$ respectively such that the ADAM scheme becomes
\begin{align}
  \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \qquad \qquad \hat{m}_t = \frac{m_t}{1-\beta^t_1}, \qquad \hat{v}_t = \frac{v_t}{1-\beta^t_2}.
  \label{eq:ADAM}
\end{align}


\subsection{Weight decay}
By adding a so-called \textit{regularization} to the loss function we can penalize high magnitudes of the model parameters. This is motivated by the idea of preventing overfitting during training. The most common way to do this is by the use of L2 regularization, adding the squared $l^2$ norm $||\theta||_2^2$, where $||\theta||_2 = \sqrt{\theta_1^2 + \theta_2^2 + \ldots}$, to the model. The new loss and gradient then become
\begin{align}
  L_{l^2}(\theta) &= L(\theta) + \frac{1}{2}\lambda ||\theta||^2_2 \nonumber \\
  \nabla_\theta L_{l2}(\theta) &=  \nabla_\theta L(\theta) + \lambda \theta,
  \label{eq:l2_grad}
\end{align}
where $\lambda$ is the weight decay parameter $\in [0,1]$. The name \textit{weight decay} relates to the fact that some practitioners only apply this penalty to the weights in the model, but we will include the biases as well (standard in PyTorch). Following the original gradient descent scheme \cref{eq:l2_grad} we get
\begin{align*}
  \theta_{t+1} &= \theta_t - \eta g_t - \eta\lambda \theta_t \\
  &= \theta_t\underbrace{(1-\eta\lambda)}_{\text{Weight decay}} - \eta g_t.
\end{align*}
Thus we notice that choosing a high weight decay (towards 1) will downscale the model parameters while choosing a low weight decay (towards 0) yields the original gradient descent scheme. Note that this is used in combination with ADAM, but it is easier to show the consequences for the original gradient descent scheme.  



\subsection{Parameter distributions}
In order to get optimal training conditions it has been found that the initial
state of the weight and biases are important \hl{SOURCE}. First of all, we must
initialize the weight by sampling from some distribution. If the weights are set
to equal values the gradient across a layer would be the same. This results in a
complexity reduction as the model can only encode the same values across the
layer \hl{SOURCE}. Further, we want to consider the gradient flow during training. Especially for deep networks, networks with many layers, we must pay attention to the problem of vanishing or exploding gradients. If we for instance consider the sigmoid activation function and its derivative 
\begin{align*}
  f(z) = \frac{1}{1 + e^{-z}}, \qquad f'(z) = \frac{df(z)}{dz} = \frac{e^{-z}}{(1+e^{-z})^2} = \frac{e^{z}}{(1+e^{z})^2},
\end{align*}
we notice that for large and small values we get $f(z\to \pm\infty) \to 0$.
However, even a small gradient will vanish throughout a deep network as the
calculation of the gradient involves the chain rule. A similar problem can be
found with the ReLU activation function which gets a zero gradient for inputs
$z<0$ which can be mitigated by the so-called leaky RelU which gives the $z<0$ a
small slope. On the other hand, we have exploding gradients, which are simply a
result of the chain rule gradient calculation. For a sufficiently deep network,
the gradient can grow exponentially and sometimes result in numerical overflow.
While there exist techniques to accommodate this, like for instance the leaky
ReLU for the vanishing gradients and so-called gradient clipping, cutting off
the gradient at a maximum, they both benefit from a properly initialized set of
weights \hl{SOURCE}. That is, we want the gradients across a given layer to have
a zero mean while the variance is similar to other layers. This balanced
gradient flow is more likely to happen if we initialize the weight by the same
set of criteria. The specific actions to achieve this depend on model architecture, including the choice of activation functions. For instance, using the ReLU activation functions it was found that the single standard deviation will depend on the number of input nodes from the previous layer $N^{[l-1]}$ as $\sim \sqrt{N^{[l-1]}}/\sqrt{2}$ and thus we simply scale a zero mean uniform distribution to match this.  This is part of the Kaiming initialization scheme which is standard in Pytorch \hl{SOURCe}. \hl{Mention choices for bias initialization}.


Batch normalization is another technique that might also help reduce the issue of poor gradient flow. Furthermore, can benefit by speeding up convergence making it more stable \hl{SOURCE}. In general, model parameters are modified throughout training meaning that the range of values coming from a previous layer will shift, even though the same training data is fed through the network repeatedly.  By scaling the input for a given layer, for each mini-batch, we can mitigate this problem and make for a more standardized input range. This often result in a faster training convergence. For layer $l$ we calculate the mean $\mu^{[l]}$ and variance $\sigma^{2[l]}$ across the layer with nodes $x_1^{[l]}, x_2^{[l]}, \ldots, x_d^{[l]}$ for each mini-batch of size $m$ as
\begin{align*}
  \mu^{[l]} = \frac{1}{m} \sum_i z^i, \qquad \sigma^{2[l]} = \frac{1}{m} \sum_i^d (x^i-\mu)^2.
\end{align*}
We then perform normal scaling of the inputs within the batch
\begin{align*}
  \hat{x}_i^{[l]} = \frac{x_i^{[l]} - \mu^{[l]}}{\sqrt{\sigma^{2[l]} + \epsilon}},
\end{align*}
where $\epsilon$ is a small number to ensure numerical stability (similar to what we used for RMSProp gradient descent). In the final step the input values are rescaled as
\begin{align*}
  \tilde{x_i} = \gamma^{[l]} \hat{x}_i^{[l]} + \beta^{[l]}
\end{align*}
with trainable parameters $\gamma$ and $\beta$. \hl{Comment about the reason for the final step}.


\subsection{Learning rate decay strategies}
Until now we have assumed a constant learning rate, but variations use a
changing learning rate beyond the adaptiveness included in the optimizers as
described earlier. It can be beneficial to start with a higher learning rate to
speed up the initial part of training and then lower the learning rate for the
final gradient descent. One straightforward strategy is a step-wise learning
rate decay where the learning rate is reduced by a factor $\gamma \in (0,1)$
every $K$ steps. A more smooth change can be achieved by for instance a
polynomial decay $\eta_t = \eta_0/t^{\alpha}$ for $\alpha > 0$. More advanced
approaches use multiple cycles of increasing and decreasing cycles. We will
mainly concern ourselves with a one-cycle policy for which we start at an
intermediate value, increase toward a maximum bound and then decrease toward a
final lower learning rate bound. This can simply be done by a linear increase
and decrease, but we choose a cosine function that is shifted to peak for the
first 30\% of the training length. 



\section{Convolutional Neural Network}
Convolutional Neural Networks (\acrshort{CNN}s) build upon some of the same
concepts as introduced with the feed-forward neural network. The difference lies
in its specialization for a spatially correlated input, such as pixels in an
image. In a dense neural network, every node is connected to each of the nodes
from the previous layers. Thus, it does not matter how the input is arranged
initially, but it cannot be changed at a later stage. This is
impractical if we want the model to recognize images of animals as the
animal-related pixels will be in different regions of each image. The
\acrshort{CNN} can be motivated by the idea of capturing spatial relations, but without being sensitive to the relative placement within the image, i.e.\ being
translational invariant. This is achieved by having a so-called \textit{kernel}
or \textit{filter} which slides over the images\footnote{Note,
that we will be using the word ``image'' as a reference for a spatially
dependent input, but in reality, it does not have to be an actual image in the
classical sense.} as it processes the input. A convolutional layer contains multiple kernels, each
consisting of a set of trainable weights and a bias. Each kernel will produce a separate output channel to the resulting \textit{feature map}. The kernel has a spatial size, specific to the model architecture, and a depth that matches the number of input channels to the layer. For instance, a typical RGB will have three channels. The kernel lines up with the image and calculates the feature map output as a dot product between the weights in the kernel and the aligning subset of the input. This is done for each input channel and summed up with the addition of a bias as illustrated in \cref{fig:conv_calculation}. The kernel then slides over by a step size given by the \textit{stride} model parameter and repeat the calculation. Choosing a stride of 2 or higher results in a reduction of spatial size. If we want to preserve the spatial size we must keep a stride of one and
additionally apply \textit{padding} to the input images, such that we can
achieve one kernel position for each input ``pixel''. The spatial size of the
feature map is given as
\begin{align}
  N_d^{[l+1]} = \left\lfloor \frac{N_d^{[l]} - F_d + 2P}{S} + 1 \right\rfloor,
  \label{eq:down_scaling}
\end{align}
for padding $P$, stride $S$, spatial size of the kernel filter $F_d$, spatial size
of input $N_d^{[l]}$, for dimension $d = {x, y}$ and layer $l$. The \textit{down-sampling} is often done through a pooling layer. A pooling layer is remniscent of a kernel, but instead of calculating the output as a dot product, it calculate the mean (mean pooling) or the max value (max pooling) of the values within its range. For instance, by using a max pooling of size $2 \times 2$ and stride two we essentially half the dimensions of the image as dictated by \cref{eq:down_scaling}. \acrshort{CNN}s will often use repeating series of convolution, pooling and then an activation function. Most architectures will down-sample the spatial input while increasing the number of channels in the model.


\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.26\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/conv_kernel_movement.png}
    % \includegraphics[width=\textwidth]{figures/theory/conv_input_channels.png}
    \caption{}
    % \label{fig:}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.70\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/conv_calculation.png}
    \caption{}
    \label{fig:conv_calculation}
  \end{subfigure}
  \hfill
  % https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53
  \caption{\hl{TMP}}
  \label{fig:conv_example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/CNN.png}
  \caption{\hl{TMP} Not used at the moment...}
  % https://www.researchgate.net/figure/Representation-of-a-Convolutional-Neural-Network-The-CNN-performs-automatic-spatial_fig3_359896672
  \label{fig:CNN}
\end{figure}


For a \acrshort{CNN}, we often consider the \textit{receptive field}. The
receptive field relates to the spatial size of the input that affects a given
node in the feature map at a given layer of the model. In
\cref{fig:receptive_field} the receptive field is illustrated for a 1D
representation of a \acrshort{CNN} with repative use of a kernel of with 2 and  stride 1. Going from the output and backward, we see that the output layers are connected to two nodes in the previous layer. Each of these nodes is connected to two nodes in the layer before that, however with one of them being the same due to the stride of 1. By back-tracking to the input we see that this
corresponds to a receptive field of $D = 5$. By increasing the filter size and
the stride the 2D receptive field will grow a lot faster than shown in this
simple 1D example. For a receptive field $D_d$, with respect to the spatial dimension $d$, a
spatial size of the filter $F_d$, stride $S_l$ (from layer $l-1$ to $l$) we have
\begin{align*}
    D_l = D_{l-1} + \left[(F_l - 1) \cdot \prod_{i=1}^{l-1}S_i \right],
\end{align*}
with $D_0 = 1$ and $l=0$ as the input layer. Note that by convention, the
product of zero elements is 1, such that for the first layer, the product is 1. The receptive field is important in understanding the connectivity in the model. The model output will be completely independent of the inputs and feature maps outside the receptive field. Furthermore, we differentiate between the theoretical
receptive field and the effective receptive field. The effective receptive field
will have a Gaussian distribution within the theoretical receptive field due to the fact that these nodes in the center of the receptive field will have more connections leading to the output, as seen in \cref{fig:receptive_field}. Thus, in practice the effective receptive field will be smaller than the theoretical Implementations like dilated convolutions, which make the filter expand in circumference and skipping skipping positions within the filter, which can be used to further increase the effective field. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/theory/receptive_field.png}
  \caption{\hl{TMP}}
  % https://lukeguerdan.com/blog/2019/intro-to-tcns/
  \label{fig:receptive_field}
\end{figure}


On a final note regarding the \acrshort{CNN} we point out that convolution is often used in combination with a dense network, or \textit{fully connected}, at the end. We can then think of the convolution part to handle the translation from a spatial import to some internal features. For the animal detection network, we would perhaps suggest features like, number of legs, size, color and so on. In practice, the network will not create easily interpreted features for the processing of the fully connected layer. We discuss one approach for interpreting of the model internals in \cref{sec:explanation}

\subsection{Training, validation and test data}
So far, we have simply considered the concept of training data as a means to
update the model parameters. Yet, we want to evaluate the model performance is
it improves. If a model has enough complexity, i.e.\ enough layers with a
sufficient amount of nodes, it can be proven that it can fit any function
\hl{SOURCE}. Thus for a complex model, it is just a matter of time before the
model eventually will learn to give matching predictions for the training data.
However, we want the model to learn general trends and not to ``memorize'' all
the data points which is known as overfitting. While the predictions can get
arbitrarily good the performance on unseen data within the same task will yield
awful results if the model is overfitted. Hence, we put aside some subset of the
data which we use as a \textit{validation} throughout and after training. By
keeping this \textit{validation} set separate from the training data we can get
a more reliable performance estimate for the model. This splitting of data
should be done randomly in order to ensure a similar distribution of data
features in each set. For the size ratio we want to strike a balance between
training quality and validation quality, usually a 20\%--80\% split favoring the
training set is a good figure. Other techniques exist which aim to optimize the
data use for sparse data situations, like cross-validation and bootstrap
\hl{fact check}, but we will not consider such methods for this thesis. A third
training set which is often forgoten is the \textit{test} set. While the
validation set should be kept unseen from the model training, the test set
should be kept unseen from the model developer. As we choose the model
architecture and hyper-parameters like learning rate, momentum and weight decay,
we will use the performance on the validation set as a metric to steer by. This
represents a slower pace parameter fitting which can also led to overfitting on
a higher level. Hence, we should denote a test set for the final evaluation of
our model which have not be considered before the end. Formally, this is the
only reliable performance metric for the model. 



\section{Overfitting and underfitting}
% THE UNDERFITTING AND OVERFITTING TRADE-OFF \cite{smith2018disciplined}. 

Underfitting and overfitting represent a crucial balance going on when training
a model. This concept is highly related to the model complexity and the chosen
hyper-parameters such as learning rate, momentum and weight decay. The textbook
visualization of underfitting and overfitting is shown in
\cref{fig:fitting_vs_time}. As we begin to train or model both the training and
validation loss is decreasing. At some point, the model will start to pick up to
specific trends which marks the transition into overfitting where the validation
loss will start to increase again. Notice that the performance on the training
data will keep increasing.  \textit{Early stopping} can be utilized to detect
this transition and stop the training. We will use a variation of this which is
based on storing the model parameters at the best validation performance but
letting the model training go on. We can imagine a similar curve as seen in
\cref{fig:fitting_vs_time} with the replacement of \textit{model complexity} on
the x-axis. For a certain amount of epochs a simple model will yield
underfitting and an overly complex model will yield overfitting. In figure
\cref{fig:fitting_quality} we see how under- and overfitting can be represented
by a fitting to a 2D curve. We see how a simple, not complex, underfitted model
will make a crude approximation for the true curve. An overly complex
overfitted model will follow pick up the noise in the training data and miss the
general trend. In practice the diagnosticating of under. and overfitting is not
as simple as the figures in \cref{fig:over_under_fitting} imply. First of all,
the \cref{fig:fitting_vs_time} is simplifying the idea, and the training and
validation loss will rarely showcase such a clear indication of the transition
to overfitting. The problem with \cref{fig:fitting_quality} lies in the fact
that we do not know the true curve. If we did, we would not need machine
learning to estimate it in the first place. Without having additional insight into
the governing source of the data the overfitting case seems to produce the most
confident fit.


\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.42\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/fitting_vs_time.png}
    \caption{}
    \label{fig:fitting_vs_time}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.57\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/theory/fitting_quality.png}
    \caption{}
    \label{fig:fitting_quality}
  \end{subfigure}
  \hfill
  \caption{\hl{TMP}}
  \label{fig:over_under_fitting}
\end{figure}




\section{Hypertuning}
Training a machine learning model revolves around tuning the model parameters
consisting of mainly weights and biases. By following the general idea by the
gradient descent approach we can get complex models able to give complex
predictions. However, a handful of so-called \textit{hyper-parameters} remain
for us to decide. First of all we must decide on an architecture for the model.
This includes high level considerations as for instance to use neural network or
a convolutional network, but also lower level considerations like the depth of
the model and number of nodes/channels per layer. The choice of loss function
and optimizers come with hyperparameters like learning rate, momentum and weight
decay. As N. Smith \cite{smith2018disciplined} puts it: ``Setting the
hyper-parameters remains a black art that requires years of experience to
acquire''. In the following, we will review some general approaches to the
hypertuning presented in \cite{smith2018disciplined}. The traditional way is to
perform a grid-search of multiple training sessions with different combinations
of hyper-parameters. This can be computationally expensive. In addition, hyper-parameters will depend on each other, the data and the architecture which can realistically be searched at the same time. 

N. Smith points to the fact that validation loss can by examined early on for
clues of either underfitting or overfitting. For the learning rates one can use
an increasing learning rate scheme for the so-called \textit{learning rate range
test} (LR range test). One specifies a minimum and a maximum learning rate
boundaries and a learning rate step size.  This can be done both for linear
increasing or decreasing learning rate, which proved to give similar results
\cite{smith2018disciplined} \hl{Jastrzebski et al. (2017b)}. We choose the
linear increasing version for simplicity. In the LR range test training then
starts form the lowest learning rate which is increased in small steps during a
pre-training. The stepsize can be number of mini-batch iterations, and does not
have to stick to epochs. For small learning rates the network will converge
slowly. As the learning rate apporahces an appropriate size the convergence more
strongly. Eventually the convergence will stop and the loss will increase again
as the learning rates becomes to high (\hl{show harmonic potential big learning
rate example somewhere?}). The learing rates at the point of divergence indicate an upper bound the learning rate that can be used with a cyclic learing rate scheme. One have to set the upper bound a bit lower than this to avoid divergence in practice. The steepest decline of the validation curve can be used as an estimate for the best constant learning rate choice. 



% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/theory/lr_descents.png}
%   \caption{\hl{TMP}  }
%   % https://www.javatpoint.com/gradient-descent-in-machine-learning
%   \label{fig:lr_descents}
% \end{figure}



Next up, is momentum. Momentum and learning rates effect each other and since
the learning rate is regarded as one of the most important hyper-parameters to
tune \hl{SOURCE} momentum is also important. We want to set both learning rate
and momentum as large as possible without causing instabilities to the training.
Momentum is designed to accelerate the training process but the effect on
updating the weights is of the same magnitude as learning rate as present from
\cref{eq:mom}. Experiments show that finding a the right momentum from momentum
range test is not useful. Thus, short runs with different values of momentum, on
the range 0.9--0.99, can be done to find the best choice of momentum. The
learning rate and momenmtum will however often be inversely dependent. Choosing
a high learning rate will result in a slightly lower momentum being appropriate
and vice versa. Thus, when using a cycling learning rate scheme one should adopt
a cyclic momentum scheme as well with decreasing momentum for increasing
learning rate. Choosing a lower momentum of 0.80--0.85 often gives similar stabile results.

Finally we address the weight decay. N. Smith reports that weight decay is not similar to learning rate and momentum as it is better to choose a constant weight decay through the training. This will be dependent on the model complexity and he suggest doing a grid-search for values such as 0, $\num{e-6}$, $\num{e-5}$ and $\num{e-4}$ for complex architecture and $\num{e-4}$, $\num{e-3}$ and $\num{e-2}$ for more shallow architectures. Choosing the weight decay on the scale of exponential exponents is often good enough as the sensitivity to precise factors is not found to be highly important. The optimal weight decay will depend on your learning rate and momentum scheme.






% Since learning rate is regarded as the most important hyper-parameter to tune (Bengio, 2012) \cite{smith2018disciplined}







\section{Prediction explanation}\label{sec:explanation}
% Source for reference: \cite{Selvaraju_2019}

With the rise of constantly improving machine learning models and computational
power, the capabilities of machine learning systems exceed human skills in many
domains \hl{SOURCE SOME}. However, with deep learning models, we have
essentially no insight into what kind of considerations went into a
prediction. This is known as the \textit{black box} problem of deep learning.
This has spiked interest in developing methods to unravel the inner parts of
the network. This effort lies partly in the development of better analytic tools
but also in the aim of creating simpler and more transparent models. We will be
using a deep-learning convolutional network that is not immediately
transparent. Since the feature maps in the network preserve the relative spatial
relations of the input one approach is to consult the feature maps for
information on which parts of the input it considers the most. However, this is
often not readily interpreted. 

Instead, we can follow the \textit{Grad-CAM} approach \cite{Selvaraju_2019}.
The idea is to use the gradients with respect to a given feature map. First, we
forward propagate the input through the model. Then, for each feature in the
feature map of choice, we calculate the gradients with respect to the loss of
our target output. For a classification task, one would often choose the one
that gives the highest prediction score. The gradients then provide an
indication of which features that are important for the prediction. By scaling
the feature map to the original size of the image we can overlay this as a
heatmap providing a visual clue of what the prediction is based on. We can do
this for different depths of the model and even combine the results for
multiple layers. In \cref{fig:grad_cam_example} we see how this can be used to
reveal the basis for a prediction. In this case the Grad-CAM analysis shows
the difference between a biased model considering the face, and thus the sex
of the person, to predict the correct profession as opposed to the unbiased
model looking at more relevant features like equipment and uniform.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/theory/grad_cam_example.png}
  \caption{\hl{TMP} \cite{Selvaraju_2019} }
  \label{fig:grad_cam_example}
\end{figure}




\section{Accelerated search using genetic algorithm}


\subsection{Markov-Chain Accelerated Genetic Algorithms}
% Following the article:
% Accelerated Genetic Algorithms with Markov Chains
% Guan Wang, Chen Chen, and K.Y. Szeto

\subsubsection{Talk about traditional method also?}

\subsubsection{Implementing for 1D chromosone (following article closely)}

We have the binary population matrix $A(t)$ at time (generation) $t$ consisting of $N$ rows denoting chromosones and with $L$ columns denoting the so-called locus (fixed position on a chromosome where a particular gene or genetic marker is located, wiki). We sort the matrix rowwise by the fitness of each chromosono evaluated by a fitness function $f$ such that $f_i(t) \le f_k(t)$ for $i \ge k$. We assume that there are a transistion probability between the current state $A(t)$ and the next state $A(t+1)$. We consider this transistion probability only to take into account mutation process (mutation only updating scheme). During each generation chromosones are sorted from most to least fitted. The chromosone at the $i$-th fitted place is assigned a row mutation probability $a_i(t)$ by some monotonic increasing function. This is taken to be 
\begin{align*}
  a_i(t) = 
  \begin{cases}
    (i-1)/N',& i-1 < N' \\
    1, &\text{else}
  \end{cases}
\end{align*}
for some limit $N'$ (refer to first part of article talking about this). We use $N' = N/2$. We also defines the survival probability $s_i = 1 - a_i$. In thus wau $a_i$ and $s_i$ decide together whether to mutate to the other state (flip binary) or to remain in the current state. We use $s_i$ as the statistical weight for the $i$-th chromosone given it a weight $w_i = s_i$.
\\
Now the column mutation. For each locus $j$ we define the count of 0's and 1's as $C_0(j)$ and $C_1(j)$ resepctively. These are normalized as
\begin{align*}
  n_0(j, t) = \frac{C_0(j)}{C_0(j) + C_1(j)}, \quad n_1(j, t) = \frac{C_1(j)}{C_0(j) + C_1(j)}.
\end{align*}
These are gathered into the vector $\vec{n}(j,t)=(n_0(j, t), n_1(j, t))$ which characterizes the state distribution of $j$-th locus. In order to direct the current population to a preferred state for locus $j$ we look at the highest weight of row $i$ for locus $j$ taking the value 0 and 1 respectively.
\begin{align*}
  C'_0(j) &= \max\{W_i | A_{ij} = 0; \ i = 1, \cdots, N\} \\
  C'_1(j) &= \max\{W_i | A_{ij} = 1; \ i = 1, \cdots, N\}
\end{align*}
which is normalized again
\begin{align*}
  n_0(j, t+1) = \frac{C'_0(j)}{C'_0(j) + C'_1(j)}, \quad n_1(j, t+1) = \frac{C'_1(j)}{C'_0(j) + C'_1(j)}.
\end{align*}
The vector $\vec{n}(j,t+1)=(n_0(j, t+1), n_1(j, t+1))$ then provides a direction for the population to evolve against. This characterizes the target state distribution of the locus $j$ among all the chromosones in the next generation. We have
\begin{align*}
  \begin{bmatrix}
    n_0(j, t+1) \\
    n_1(j, t+1)
  \end{bmatrix}
  = 
  \begin{bmatrix}
    P_{00}(j,t) \ P_{10}(j,t) \\
    P_{01}(j,t) \ P_{11}(j,t)
  \end{bmatrix}
  \begin{bmatrix}
    n_0(j, t) \\
    n_1(j, t)
  \end{bmatrix}
\end{align*}
Since the probability must sum to one for the rows in the P-matrix we have 
\begin{align*}
  P_{00}(j, t) = 1 - P_{01}(j, t), \quad P_{11}(j, t) = 1 - P_{10}(j, t)
\end{align*}
These conditions allow us to solve for the transition probability $P_{10}(j,t)$ in terms of the single variable $P_{00}{j,t}$.
\begin{align*}
  P_{10}(j,t) &= \frac{n_0(j, t+1) - P_{00}(j,t)n_0(j, t)}{n_1(j,t)} \\
  P_{01}(j,t) &= 1 - P_{00}(j,t) \\
  P_{11}(j,t) &= 1 - P_{10}(j,t)
\end{align*}
We just need to know $P_{00}(j,t)$. We start from $P_{00}(j, t = 0) = 0.5$ and then choose $P_{00}(j,t) = n_0(j,t)$




\subsubsection{Repair function}
Talk about it here or in random walk section?