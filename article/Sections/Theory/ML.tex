\chapter{Machine Learning}

We will use machine learning as a tool for designing kirigami configurations that alter the friction behavior in different manners. Through \acrshort{MD} simulations we will provide data for which the machine learning model can learn from. If successful we can utilize the model to predict the friction behavior of unseen configurations which makes us able to skip the \acrshort{MD} and speed up the search process. It is not obvious that the model can capture the physical mechanism governing the friction process and thus a succeeding model might invite further and more advanced study within this domain. 

We aim to implement a basic machine learning approach as an investigation whether this is useful. 

\section{Neural network}
% Feed forwatd dense neural network.
A neural network is of the foundational concepts in machine learning. It consists of an input $\vec{x}$ some so-called \textit{hidden layers} and an output $\hat{\vec{y}}$. We reserve $\vec{y}$ for the actual true output that the model is going to estimate. The input vector $\vec{x} = x_0, x_1, \ldots, x_{n_x}$ contains the input \textit{features}. Each input $x_i$ is connected to each of the \textit{nodes} in the first hidden layer. For a given note $a_j^{[1]}$ in the first hidden layer the input is processed as
\begin{align*}
  a_j^{[1]} = f(\sum_i w_{ij}x_i + b_j^{[1]}),
\end{align*}
where $w_{ij}$ is the weight representing the connection strength from the input feature $x_i$ to the (first) hidden layer node $a_j^{[1]}$, $b_j^{[1]}$ is a bias associated to the specific hidden layer node and  $f(\cdot)$ is an \textit{activation function}. Notice, that we have individual weights for each connection in the network but we ommit any notation for the layers it connects as it is obvious from the equations so far. The activation function is an crucial part of the neural network as it provides a non-linear mapping to the inputs to the node. Without this, the resulting framework is nothing more than a linear transformation. By introducing the activation function the model is capable of capturing complex trends \hl{SOURCE}. The signal is carried through the hidden layers in a similar fashion from hidden layer $l-1$ to $l$ as 
\begin{align*}
  a_j^{[l]} = f\left(\sum_i w_{ij}a_j^{[l-1]} + b_j^{[1]}\right).
\end{align*}
Keep in mind that weight is specific to the connection between layer $l-1$ and $l$. Finally, the last hidden layer, $L-1$, connects densely to the output $\hat{\vec{y}}$ as for the remaining of the network. However, here the activation function can be omitted or exchanged with a different mapping, e.g. the sigmoid which maps the output to $(0,1)$. The whole process of sending data through the model is called \textit{forward propagation} as it constitutes the process of sending information through the network. The specific values of the weights and biases will govern the final output.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/theory/ffnn.png}
  \caption{\hl{From overleaf IN400}}
  \label{fig:ffnn}
\end{figure}

The network is trained through the process of \textit{backpropagation} and \textit{gradient descent} optimization. Backpropagation starts with a definition of the error associated with the model output. A so-called \textit{loss function} $L$ compares the output $\hat{\vec{y}}$ with the expected result $\vec{y}$ and calculates the error otherwise known as the loss. For a continuous scalar output, we might simply use the mean squared error (\acrshort{MSE})
\begin{align*}
  L_{\text{\acrshort{MSE}}} = \frac{1}{N_y} \sum_{i = 1}^N (y_i - \hat{y}_i)^2.
\end{align*}
For classification, where we expect an output of True or False (1 or 0), a common choice is binary cross entropy (BSE)
\begin{align*}
  L_{\text{BSE}} =  -\sum_{i=1}^n \ \Big[y_i\log(\hat{y_i}) + (1-y_i)\log(1 - \hat{y_i}) \Big].
\end{align*}
This arrises for information theory \hl{or something similar}. Without going into details with the derivation we can convince ourselves that the error is minimized (0) for a perfect prediction and maximized for a completely wrong prediction. When $y_i = 1$ we essentially add the negative term $-\log(\hat{y_i})$. With a prediction given as a number between 0 and 1 (through the sigmoid function), we can interpret the output as a probability of the given classification. When the output approaches 1, $\hat{y}_i \to 1$, in the correct case of $y_i = 1$ the loss contribution goes to towards zero. However, if the output approaches 0, $\hat{y}_i \to 1$, the loss contribution diverges $\hat{y}_i \to \inf$. Similar apply for the case of $y_i = 0$ with opposite directions. 

Given a loss function we can calculate the loss gradient with respect to each weight and bias in the model. That is, how is the error effected is we tweak a certain model parameter, weight or bias. The overall idea is then to  ``nudge'' each parameter in the appropriate direction and repeat the process of forward and backpropagation. For a given paramter $\theta_t$ at time $t$, measured in epoch for instance (sets of forward and backward repetitions) we can update it through gradient descent 
\begin{align*}
  \theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t),
\end{align*}
where the new parameter $\theta_t$ is given by stepping in the direction of the loss gradient $\nabla_\theta L(\theta_t)$ with a step length proportional to learning rate $\eta$. In practice, gradient descent is done in batches. That is, for a batch of training data, consisting of input data and true output data, a single step is taken for the average gradient. If a small batch is considered the path towards the optimal parameters will be governed by a fluctuation in loss space while a large batch will result in a more direct path towards lower error. This is the key concept behind stochastic gradient descent (SGD) which creates random batches from the training data and performs gradient descent. Even though a stable descent toward a minimum error sounds promising, the strength in \acrshort{SGD} lies in the fluctuations of its descent. Without it, learning would be stuck at the first local minimum in the loss space. 

% Some illustration of SGD maybe?

\section{Optimizers}
% https://arxiv.org/pdf/1412.6980.pdf
The name \textit{optimizers} covers a variety of gradient descent methods which can be utilized for model training. In our study we will use the ADAM optimizer (short?) which extends the concept of stochastic gradient descent. ADAM combinnes several tricks from other succesfull optimizers by including an exponetial moving average and momentum term. 


% Momentum 
We extend the gradient descent by the introduction of a momentum term $m_t$ such that we get
\begin{align}
  \theta_{t+1} = \theta_t - m_{t+1}, \qquad m_{t+1} = \alpha m_t + \eta \nabla_\theta L(\theta_t)
  \label{eq:mom}
\end{align}
with $m_0 = 0$. If we introduce the shorthand $g_t = \nabla_\theta L(\theta_t)$ we find
\begin{align*}
  m_0 &= 0 \\
  m_1 &= \alpha m_0 + \eta g_0 = \eta g_0 \\
  m_2 &= \alpha m_1 + \eta g_1 = \alpha^1 \eta g_0 + \eta g_1 \\
  m_2 &= \alpha m_2 + \eta g_2 = \alpha^2 \eta g_0 + \alpha\eta g_1 + \eta g_2 \\
  &\vdots \\
  m_t &= \eta \left(\sum_{i=0}^{t-1} \alpha^{t-1-i}g_i\right).
\end{align*}
Hence $m_t$ is a weighted average of the gradients which an exponentially decreasing weight. This act as a memory of the previous gradients and aid in passing of local minema of plateaus in the loss space. A variation of momentum can be achieved with the introduction of the exponetial moving average (EMA)
\begin{align*}
    \text{EMA}(g_0) &= 0 + (1-\alpha)g_0 \\
    \text{EMA}(g_1) &= \alpha \text{EMA}(g_t) + (1-\alpha)g_1 \\
    &\vdots \\
    \text{EMA}(g_t) &= \alpha \text{EMA}(g_{t-1}) + (1-\alpha)g_t  = \sum_{s=0}^t \alpha^{t-s}(1-\alpha)g_t,
\end{align*}
which is quite similar to that of momentum, but here we have the explicit weighting by $(1-\alpha)$.


% RMSProp
Another trick in the book is the root mean square propagation (RMSProp) which is motivated by the problem of passing long plateaus in the loss space. Since the size of the updates are proportional to the norm of the gradient
\begin{align*}
  \theta_{t+1} = \theta_t - \eta g_t \ \Longrightarrow \ ||\theta_{t+1}-\theta_{t}|| = \eta ||g_t||
\end{align*}
we might get the idea of normalizing the gradient step by dividing with the norm $|||g_t|$.  However, this does not immediately solve the problem of long plateaus as we need to consider multiple past gradients, and thus we use the EMA instead. When reentering a steep region again we need to quickly downscale the gradient steps. By using the squared norm $||g_t||^2$ for the EMA we make it more sensitive to outliers and achieve just that. This corresponds to the RMSProp update scheme
\begin{align}
  \theta_{t+1} = \theta_t - \eta \frac{g_t}{\sqrt{\text{EMA}(||g_t||^2)} + \epsilon},
  \label{eq:RMSProp}
\end{align}
where $\epsilon$ is simply a small number to avoid division by issues. ADAM merges the idea behind the root mean square propagation technique in \cref{eq:RMSProp} with a EMA momentum term $m_t$ and second order RMSProp term $v_t$ 
\begin{align*}
  m_t &= \beta_1 m_{t_1} + (1-\beta_1)g_t, \\
  v_t &= \beta_2 v_{t_1} + (1-\beta_2)g_t^2, 
\end{align*}
These are initially set to zero and will be biased toward that. ADAM correct for this by applying a scaling term $(1-\beta^t_1)$ and $(1-\beta^t_2)$ respectively
\begin{align}
  \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \qquad \qquad \hat{m}_t &= \frac{m_t}{1-\beta^t_1}, \qquad \hat{v}_t = \frac{v_t}{1-\beta^t_2} \\
  \label{eq:ADAM}
\end{align}


\subsection{Weight decay}
By adding a $l^2$ norm of the parameters to the cost function we get a penalty for high parameters. 


\subsection{Batch normalization}

\subsection{Learning rate decay strategies}
\section{Convolutional neural network}


\section{Hypertuning strategies}

\section{Prediction explanation}
Looking at feature maps and gradient maps.


% \begin{itemize}
%   \item Feed forward fully connected
%   \item CNN
%   \item GAN (encoder + decoder)
%   \item Genetic algorithm
%   \item Using machine learning for inverse designs partly eliminate the black
%   box problem. When a design is produced we can test it, and if it works we not
%   rely on machine learning connections to verify it's relevance. 
%   \item However, using explanaitons techniques such as maybe t-SNE, Deep dream,
%   LRP, Shapley values and linearizations, we can try to understand why the AI
%   chose as it did. This can lead to an increased understanding of each design
%   feature. Again this is not dependent on the complex network of the network as
%   this can be tested and veriied independently of the network. 
% \end{itemize}

% \section{Feed forward network / Neural networks}
% \section{CNN for image recognition}
% \section{GAN (encoder + deoder)}
% \section{Inverse desing using machine learning}
% \section{Prediction explanation}
% \subsection{Shapley}
% \subsection{Lineariations}
% \subsection{LRP}
% \subsection{t-SNE}

\section{Accelerated search using genetic algorithm}


\subsection{Markov-Chain Accelerated Genetic Algorithms}
% Following the article:
% Accelerated Genetic Algorithms with Markov Chains
% Guan Wang, Chen Chen, and K.Y. Szeto

\subsubsection{Talk about traditional method also?}

\subsubsection{Implementing for 1D chromosone (following article closely)}

We have the binary population matrix $A(t)$ at time (generation) $t$ consisting of $N$ rows denoting chromosones and with $L$ columns denoting the so-called locus (fixed position on a chromosome where a particular gene or genetic marker is located, wiki). We sort the matrix rowwise by the fitness of each chromosono evaluated by a fitness function $f$ such that $f_i(t) \le f_k(t)$ for $i \ge k$. We assume that there are a transistion probability between the current state $A(t)$ and the next state $A(t+1)$. We consider this transistion probability only to take into account mutation process (mutation only updating scheme). During each generation chromosones are sorted from most to least fitted. The chromosone at the $i$-th fitted place is assigned a row mutation probability $a_i(t)$ by some monotonic increasing function. This is taken to be 
\begin{align*}
  a_i(t) = 
  \begin{cases}
    (i-1)/N',& i-1 < N' \\
    1, &\text{else}
  \end{cases}
\end{align*}
for some limit $N'$ (refer to first part of article talking about this). We use $N' = N/2$. We also defines the survival probability $s_i = 1 - a_i$. In thus wau $a_i$ and $s_i$ decide together whether to mutate to the other state (flip binary) or to remain in the current state. We use $s_i$ as the statistical weight for the $i$-th chromosone given it a weight $w_i = s_i$.
\\
Now the column mutation. For each locus $j$ we define the count of 0's and 1's as $C_0(j)$ and $C_1(j)$ resepctively. These are normalized as
\begin{align*}
  n_0(j, t) = \frac{C_0(j)}{C_0(j) + C_1(j)}, \quad n_1(j, t) = \frac{C_1(j)}{C_0(j) + C_1(j)}.
\end{align*}
These are gathered into the vector $\vec{n}(j,t)=(n_0(j, t), n_1(j, t))$ which characterizes the state distribution of $j$-th locus. In order to direct the current population to a preferred state for locus $j$ we look at the highest weight of row $i$ for locus $j$ taking the value 0 and 1 respectively.
\begin{align*}
  C'_0(j) &= \max\{W_i | A_{ij} = 0; \ i = 1, \cdots, N\} \\
  C'_1(j) &= \max\{W_i | A_{ij} = 1; \ i = 1, \cdots, N\}
\end{align*}
which is normalized again
\begin{align*}
  n_0(j, t+1) = \frac{C'_0(j)}{C'_0(j) + C'_1(j)}, \quad n_1(j, t+1) = \frac{C'_1(j)}{C'_0(j) + C'_1(j)}.
\end{align*}
The vector $\vec{n}(j,t+1)=(n_0(j, t+1), n_1(j, t+1))$ then provides a direction for the population to evolve against. This characterizes the target state distribution of the locus $j$ among all the chromosones in the next generation. We have
\begin{align*}
  \begin{bmatrix}
    n_0(j, t+1) \\
    n_1(j, t+1)
  \end{bmatrix}
  = 
  \begin{bmatrix}
    P_{00}(j,t) \ P_{10}(j,t) \\
    P_{01}(j,t) \ P_{11}(j,t)
  \end{bmatrix}
  \begin{bmatrix}
    n_0(j, t) \\
    n_1(j, t)
  \end{bmatrix}
\end{align*}
Since the probability must sum to one for the rows in the P-matrix we have 
\begin{align*}
  P_{00}(j, t) = 1 - P_{01}(j, t), \quad P_{11}(j, t) = 1 - P_{10}(j, t)
\end{align*}
These conditions allow us to solve for the transition probability $P_{10}(j,t)$ in terms of the single variable $P_{00}{j,t}$.
\begin{align*}
  P_{10}(j,t) &= \frac{n_0(j, t+1) - P_{00}(j,t)n_0(j, t)}{n_1(j,t)} \\
  P_{01}(j,t) &= 1 - P_{00}(j,t) \\
  P_{11}(j,t) &= 1 - P_{10}(j,t)
\end{align*}
We just need to know $P_{00}(j,t)$. We start from $P_{00}(j, t = 0) = 0.5$ and then choose $P_{00}(j,t) = n_0(j,t)$




\subsubsection{Repair function}
Talk about it here or in random walk section?